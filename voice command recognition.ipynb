{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d83004f-3c37-42fc-81a9-14e53afa9e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Size: 28536\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "def get_dataset_size_and_labels(data_path):\n",
    "    class_labels = []\n",
    "    dataset_size = 0\n",
    "    for folder in os.listdir(data_path):\n",
    "        folder_path = os.path.join(data_path, folder)\n",
    "        if os.path.isdir(folder_path):\n",
    "            class_labels.append(folder)\n",
    "            for filename in os.listdir(folder_path):\n",
    "                dataset_size += 1\n",
    "    return dataset_size, class_labels\n",
    "data_path = r\"C:\\Users\\SRINI\\Downloads\\archive (3)\"\n",
    "dataset_size, class_labels = get_dataset_size_and_labels(data_path)\n",
    "print(\"Dataset Size:\", dataset_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a619650-f2b9-4a75-bae0-79f0c3da33cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (22828, 13, 20)\n",
      "X_test shape: (5707, 13, 20)\n",
      "Number of classes: 15\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "def load_and_preprocess_data(data_path, sample_rate=16000, duration=1):\n",
    "    labels = []\n",
    "    features = []\n",
    "    max_pad_len = 20 \n",
    "    class_labels = os.listdir(data_path)\n",
    "    for label in class_labels:\n",
    "        label_path = os.path.join(data_path, label)\n",
    "        for audio_file in os.listdir(label_path):\n",
    "            audio_path = os.path.join(label_path, audio_file)\n",
    "            if not audio_file.endswith(('.wav', '.mp3', '.flac')):\n",
    "                continue\n",
    "            try:\n",
    "                audio, _ = librosa.load(audio_path, sr=sample_rate, duration=duration)\n",
    "                mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=13)\n",
    "                if mfccs.shape[1] > max_pad_len:\n",
    "                    mfccs = mfccs[:, :max_pad_len]\n",
    "                else:\n",
    "                    pad_width = max_pad_len - mfccs.shape[1]\n",
    "                    mfccs = np.pad(mfccs, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
    "                features.append(mfccs)\n",
    "                labels.append(label)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {audio_path}: {e}\")\n",
    "    return np.array(features), np.array(labels)\n",
    "data_path = r\"C:\\Users\\SRINI\\Downloads\\archive (3)\"\n",
    "X, y = load_and_preprocess_data(data_path)\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"Number of classes:\", len(np.unique(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da2baee9-5438-455d-be55-b6d4aafcc264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "714/714 [==============================] - 8s 9ms/step - loss: 2.9905 - accuracy: 0.0804 - val_loss: 2.6648 - val_accuracy: 0.0801\n",
      "Epoch 2/10\n",
      "714/714 [==============================] - 5s 7ms/step - loss: 2.6569 - accuracy: 0.0834 - val_loss: 2.6494 - val_accuracy: 0.0808\n",
      "Epoch 3/10\n",
      "714/714 [==============================] - 5s 8ms/step - loss: 2.6463 - accuracy: 0.0796 - val_loss: 2.6419 - val_accuracy: 0.0799\n",
      "Epoch 4/10\n",
      "714/714 [==============================] - 6s 8ms/step - loss: 2.6405 - accuracy: 0.0839 - val_loss: 2.6376 - val_accuracy: 0.0806\n",
      "Epoch 5/10\n",
      "714/714 [==============================] - 6s 9ms/step - loss: 2.6371 - accuracy: 0.0806 - val_loss: 2.6348 - val_accuracy: 0.0799\n",
      "Epoch 6/10\n",
      "714/714 [==============================] - 5s 7ms/step - loss: 2.6349 - accuracy: 0.0806 - val_loss: 2.6330 - val_accuracy: 0.0799\n",
      "Epoch 7/10\n",
      "714/714 [==============================] - 5s 7ms/step - loss: 2.6334 - accuracy: 0.0827 - val_loss: 2.6316 - val_accuracy: 0.0799\n",
      "Epoch 8/10\n",
      "714/714 [==============================] - 6s 8ms/step - loss: 2.6324 - accuracy: 0.0814 - val_loss: 2.6308 - val_accuracy: 0.0799\n",
      "Epoch 9/10\n",
      "714/714 [==============================] - 5s 7ms/step - loss: 2.6317 - accuracy: 0.0806 - val_loss: 2.6299 - val_accuracy: 0.0799\n",
      "Epoch 10/10\n",
      "714/714 [==============================] - 5s 8ms/step - loss: 2.6312 - accuracy: 0.0828 - val_loss: 2.6296 - val_accuracy: 0.0799\n",
      "179/179 [==============================] - 1s 3ms/step - loss: 2.6296 - accuracy: 0.0799\n",
      "Test Accuracy: 7.99%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "def load_and_preprocess_data(data_path, sample_rate=16000, duration=1):\n",
    "    labels = []\n",
    "    features = []\n",
    "    max_pad_len = 20  \n",
    "    class_labels = os.listdir(data_path)\n",
    "    for label in class_labels:\n",
    "        label_path = os.path.join(data_path, label) \n",
    "        for audio_file in os.listdir(label_path):\n",
    "            audio_path = os.path.join(label_path, audio_file)\n",
    "            if not audio_file.endswith(('.wav', '.mp3', '.flac')):\n",
    "                continue\n",
    "            try:\n",
    "                audio, _ = librosa.load(audio_path, sr=sample_rate, duration=duration)\n",
    "                mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=13)\n",
    "                if mfccs.shape[1] > max_pad_len:\n",
    "                    mfccs = mfccs[:, :max_pad_len]\n",
    "                else:\n",
    "                    pad_width = max_pad_len - mfccs.shape[1]\n",
    "                    mfccs = np.pad(mfccs, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
    "                features.append(mfccs)\n",
    "                labels.append(label)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {audio_path}: {e}\")\n",
    "    return np.array(features), np.array(labels)\n",
    "data_path = r\"C:\\Users\\SRINI\\Downloads\\archive (3)\"\n",
    "X, y = load_and_preprocess_data(data_path)\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "X = np.expand_dims(X, axis=-1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(X_train.shape[1], X_train.shape[2], 1)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(np.unique(y)), activation='softmax'))\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "epochs = 10  \n",
    "model.fit(X_train, y_train, epochs=epochs, batch_size=32, validation_data=(X_test, y_test))\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2703c71b-d488-426d-8180-ab149b8267af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179/179 [==============================] - 1s 3ms/step\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bed       1.00      0.00      0.00       325\n",
      "        bird       1.00      0.00      0.00       348\n",
      "         cat       1.00      0.00      0.00       336\n",
      "         dog       1.00      0.00      0.00       359\n",
      "        down       1.00      0.00      0.00       460\n",
      "       eight       1.00      0.00      0.00       476\n",
      "        five       1.00      0.00      0.00       512\n",
      "        four       1.00      0.00      0.00       501\n",
      "          go       0.08      1.00      0.15       456\n",
      "       happy       1.00      0.00      0.00       357\n",
      "       house       1.00      0.00      0.00       370\n",
      "        left       1.00      0.00      0.00       457\n",
      "      marvin       1.00      0.00      0.00       332\n",
      "        nine       1.00      0.00      0.00       418\n",
      "\n",
      "    accuracy                           0.08      5707\n",
      "   macro avg       0.93      0.07      0.01      5707\n",
      "weighted avg       0.93      0.08      0.01      5707\n",
      "\n",
      "Confusion Matrix:\n",
      "[[  0   0   0   0   0   0   0   0 325   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0 348   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0 336   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0 359   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0 460   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0 476   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0 512   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0 501   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0 456   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0 357   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0 370   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0 457   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0 332   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0 418   0   0   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_test_original = le.inverse_transform(y_test)\n",
    "y_pred_original = le.inverse_transform(y_pred_classes)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test_original, y_pred_original,zero_division=1))\n",
    "conf_mat = confusion_matrix(y_test_original, y_pred_original)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd4de7b2-6971-4b38-9e59-ea84d6ded928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 59ms/step\n",
      "Predicted Class: 9\n",
      "Predicted Label: go\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "def preprocess_single_audio(audio_path, sample_rate=16000, duration=1, max_pad_len=20):\n",
    "    try:\n",
    "        audio, _ = librosa.load(audio_path, sr=sample_rate, duration=duration)\n",
    "        mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=13)\n",
    "        if mfccs.shape[1] > max_pad_len:\n",
    "            mfccs = mfccs[:, :max_pad_len]\n",
    "        else:\n",
    "            pad_width = max_pad_len - mfccs.shape[1]\n",
    "            mfccs = np.pad(mfccs, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
    "        mfccs = np.expand_dims(mfccs, axis=-1)\n",
    "        return mfccs\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {audio_path}: {e}\")\n",
    "        return None\n",
    "single_audio_path = r\"C:\\Users\\SRINI\\Downloads\\archive (3)\\go\\ffd2ba2f_nohash_4.wav\"\n",
    "preprocessed_audio = preprocess_single_audio(single_audio_path)\n",
    "if preprocessed_audio is not None:\n",
    "    predictions = model.predict(np.array([preprocessed_audio]))\n",
    "    predicted_class = np.argmax(predictions)\n",
    "    predicted_label = le.inverse_transform([predicted_class])[0]\n",
    "    print(f\"Predicted Class: {predicted_class}\")\n",
    "    print(f\"Predicted Label: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8265aff-e06e-4647-b044-c5e742e50a2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
